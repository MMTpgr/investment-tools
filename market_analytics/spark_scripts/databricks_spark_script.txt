from pyspark.sql.functions import col, when


# File location and type
file_location = "/FileStore/tables"
file_type = "parquet"


# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type).load(file_location)

to_drop = ['minTicks','industryGroup','industrySubGroup','isQuotable','tradeUnit']

for collumn in to_drop:
    df = df.drop(collumn)

# eliminate null dividend or yield rows
df = df.filter(
    (col('dividend')>0) 
    
)

# Calculate dividend frequency
adjusted_yield = col('yield') - (col('dividend') * 1200) / col('prevDayClosePrice')

df = df.withColumn(
    'dividend frequency',
    when(
        col('dividend') != 0,
        when(
            (adjusted_yield > -1) & (adjusted_yield < 1),
            'monthly'
        ).otherwise('quarterly')
    ).otherwise(None)
)


# Calculate payout ratio
df = df.withColumn(
    'payout ratio',
    col('eps') / when(
        col('dividend frequency') == 'monthly',
        col('dividend') * 12
    ).otherwise(col('dividend') * 4)
)

# Keep only monthly dividend stocks
# df = df.where(col('dividend frequency') == 'quarterly')

# Apply final filters
df = df.filter(
    (col('yield') >= 4) &
    (col('industrySector') != 'FinancialServices') 
)

df = df.orderBy(col('payout ratio').asc()).orderBy(col('dividend frequency'))

df.write.format("parquet").save("/FileStore/production_magitech_core")

print(df.count())

display(df)